{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:30.592891Z",
     "start_time": "2025-03-13T22:41:30.066339Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:31.105282Z",
     "start_time": "2025-03-13T22:41:30.597560Z"
    }
   },
   "outputs": [],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:47.658114Z",
     "start_time": "2025-03-13T22:41:31.174098Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in forksafe hook <bound method LLMObs._child_after_fork of LLMObs(status=<ServiceStatus.STOPPED: 'stopped'>, tracer=<ddtrace._trace.tracer.Tracer object at 0x1062e85f0>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yahya.mouman/Documents/VSCode/jupyter-prompts/.venv/lib/python3.12/site-packages/ddtrace/internal/forksafe.py\", line 45, in run_hooks\n",
      "    hook()\n",
      "  File \"/Users/yahya.mouman/Documents/VSCode/jupyter-prompts/.venv/lib/python3.12/site-packages/ddtrace/llmobs/_llmobs.py\", line 256, in _child_after_fork\n",
      "    self._llmobs_span_writer = self._llmobs_span_writer.recreate()\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yahya.mouman/Documents/VSCode/jupyter-prompts/.venv/lib/python3.12/site-packages/ddtrace/llmobs/_writer.py\", line 308, in recreate\n",
      "    return self.__class__(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yahya.mouman/Documents/VSCode/jupyter-prompts/.venv/lib/python3.12/site-packages/ddtrace/llmobs/_writer.py\", line 255, in __init__\n",
      "    raise ValueError(\"agentless_url is required for agentless mode\")\n",
      "ValueError: agentless_url is required for agentless mode\n",
      "Exception ignored in forksafe hook <bound method LLMObs._child_after_fork of LLMObs(status=<ServiceStatus.RUNNING: 'running'>, tracer=<ddtrace._trace.tracer.Tracer object at 0x1062e85f0>)>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yahya.mouman/Documents/VSCode/jupyter-prompts/.venv/lib/python3.12/site-packages/ddtrace/internal/forksafe.py\", line 45, in run_hooks\n",
      "    hook()\n",
      "  File \"/Users/yahya.mouman/Documents/VSCode/jupyter-prompts/.venv/lib/python3.12/site-packages/ddtrace/llmobs/_llmobs.py\", line 256, in _child_after_fork\n",
      "    self._llmobs_span_writer = self._llmobs_span_writer.recreate()\n",
      "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yahya.mouman/Documents/VSCode/jupyter-prompts/.venv/lib/python3.12/site-packages/ddtrace/llmobs/_writer.py\", line 308, in recreate\n",
      "    return self.__class__(\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/yahya.mouman/Documents/VSCode/jupyter-prompts/.venv/lib/python3.12/site-packages/ddtrace/llmobs/_writer.py\", line 255, in __init__\n",
      "    raise ValueError(\"agentless_url is required for agentless mode\")\n",
      "ValueError: agentless_url is required for agentless mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/DataDog/dd-trace-py.git@yahya/update-prompt-annotation\n",
      "  Cloning https://github.com/DataDog/dd-trace-py.git (to revision yahya/update-prompt-annotation) to /private/var/folders/fw/0099t8y529b2x_y472l_vnr40000gp/T/pip-req-build-ag4wo68j\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/DataDog/dd-trace-py.git /private/var/folders/fw/0099t8y529b2x_y472l_vnr40000gp/T/pip-req-build-ag4wo68j\n",
      "  Running command git checkout -b yahya/update-prompt-annotation --track origin/yahya/update-prompt-annotation\n",
      "  Switched to a new branch 'yahya/update-prompt-annotation'\n",
      "  branch 'yahya/update-prompt-annotation' set up to track 'origin/yahya/update-prompt-annotation'.\n",
      "  Resolved https://github.com/DataDog/dd-trace-py.git to commit f07052bef095f8f34228c1025a85166bdea79c2d\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bytecode>=0.15.1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev49+gf07052bef) (0.16.1)\n",
      "Requirement already satisfied: envier~=0.6.1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev49+gf07052bef) (0.6.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev49+gf07052bef) (1.30.0)\n",
      "Requirement already satisfied: protobuf>=3 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev49+gf07052bef) (6.30.0)\n",
      "Requirement already satisfied: typing_extensions in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev49+gf07052bef) (4.12.2)\n",
      "Requirement already satisfied: xmltodict>=0.12 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev49+gf07052bef) (0.14.2)\n",
      "Requirement already satisfied: wrapt>=1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev49+gf07052bef) (1.17.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1->ddtrace==3.4.0.dev49+gf07052bef) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1->ddtrace==3.4.0.dev49+gf07052bef) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1->ddtrace==3.4.0.dev49+gf07052bef) (3.21.0)\n",
      "Building wheels for collected packages: ddtrace\n",
      "  Building wheel for ddtrace (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ddtrace: filename=ddtrace-3.4.0.dev49+gf07052bef-cp312-cp312-macosx_14_0_arm64.whl size=8540944 sha256=4b9eb3eefb92bcbc88197f9b89457cee4c069ca2e71322dbdf87fdb478964a43\n",
      "  Stored in directory: /private/var/folders/fw/0099t8y529b2x_y472l_vnr40000gp/T/pip-ephem-wheel-cache-ub5140z8/wheels/55/fd/11/cfb8de599ac245987c278626cc41e148a13c4b25bb618ed43b\n",
      "Successfully built ddtrace\n",
      "Installing collected packages: ddtrace\n",
      "  Attempting uninstall: ddtrace\n",
      "    Found existing installation: ddtrace 3.2.0.dev29+g6019952bd\n",
      "    Uninstalling ddtrace-3.2.0.dev29+g6019952bd:\n",
      "      Successfully uninstalled ddtrace-3.2.0.dev29+g6019952bd\n",
      "Successfully installed ddtrace-3.4.0.dev49+gf07052bef\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/DataDog/dd-trace-py.git@yahya/update-prompt-annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:48.038521Z",
     "start_time": "2025-03-13T22:41:47.663029Z"
    }
   },
   "outputs": [],
   "source": [
    "from ddtrace.llmobs import LLMObs\n",
    "from ddtrace.llmobs.utils import Prompt\n",
    "from ddtrace import patch\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "patch(openai=True)\n",
    "\n",
    "openai.api_key = os.environ.get(\"openai_api_key\")\n",
    "client = OpenAI()\n",
    "load_dotenv()\n",
    "\n",
    "LLMObs.enable(\n",
    "    api_key=os.environ.get(\"dd_api_key\"),\n",
    "    site=os.environ.get(\"dd_site\"),\n",
    "    ml_app=\"prompt-tracking-sandbox-v3\",\n",
    "    service=\"prompt-tracking-sandbox-v3\",\n",
    "    agentless_enabled=True,\n",
    "    env=\"testing\",\n",
    "    integrations_enabled=False,\n",
    ")\n",
    "\n",
    "prompt_id = \"prompt-tracking-sandbox-prompt\"\n",
    "prompt_name = \"recommendation_engine\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:48.045934Z",
     "start_time": "2025-03-13T22:41:48.043179Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_openai(system_prompt=None, user_prompt=None, variables=[], model=\"gpt-4o\", max_tokens=100, temperature=0.7):\n",
    "     \"\"\" Sends a prompt to the OpenAI API and returns the text response. \"\"\" \n",
    "     if system_prompt and user_prompt:\n",
    "          system_prompt_rendered = system_prompt.format(**variables)\n",
    "          user_prompt_rendered = user_prompt.format(**variables)\n",
    "          completion = client.chat.completions.create(\n",
    "               model=model,\n",
    "               max_tokens=max_tokens,\n",
    "               temperature=temperature,\n",
    "               messages=[{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_rendered\n",
    "                    },\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt_rendered\n",
    "                    }]\n",
    "          )\n",
    "     elif user_prompt:\n",
    "          user_prompt_rendered = user_prompt.format(**variables)\n",
    "          completion = client.chat.completions.create(\n",
    "               model=model,\n",
    "               max_tokens=max_tokens,\n",
    "               temperature=temperature,\n",
    "               messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt_rendered\n",
    "                    }]\n",
    "          )\n",
    "     elif system_prompt:\n",
    "            system_prompt_rendered = system_prompt.format(**variables)\n",
    "            completion = client.chat.completions.create(\n",
    "                 model=model,\n",
    "                 max_tokens=max_tokens,\n",
    "                 temperature=temperature,\n",
    "                 messages=[{\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt_rendered\n",
    "                        }]\n",
    "            )\n",
    "     return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to annotate a prompt to a span \n",
    "In order to trace the prompt we wrap the openai call with LLMObs.prompt_context or LLMObs.annotation_context.\n",
    "Example :\n",
    "\n",
    "\"\"\"\n",
    "with LLMObs.prompt_context(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.0.0\",\n",
    "                           template=[(\"User\", prompt_step1)], \n",
    "                           variables=variables_step1\n",
    "                           ):\n",
    "\"\"\"\n",
    "Or\n",
    "\"\"\"\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id,\n",
    "                           name=prompt_name,\n",
    "                           version=\"1.0.0\",\n",
    "                           template=[(\"User\", prompt_step1)],\n",
    "                           variables=variables_step1\n",
    "                           )):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 simple user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:14.910406Z",
     "start_time": "2025-03-13T22:41:48.050376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USER Prompt Template (Step 1) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 1) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.'}\n",
      "\n",
      "=== Response (Step 1) ===\n",
      "Sure! How about visiting Kyoto, Japan? It's a city rich in history and culture, with stunning temples and beautiful gardens.\n",
      "\n",
      "For a restaurant, consider dining at \"Kikunoi,\" a renowned kaiseki restaurant in Kyoto. Kikunoi offers an exquisite multi-course dining experience that showcases the best of traditional Japanese cuisine, using seasonal and local ingredients. The ambiance and attention to detail make it a memorable experience. Be sure to make a reservation in advance!\n"
     ]
    }
   ],
   "source": [
    "prompt_step1 = \"{user_prompt}\"\n",
    "variables_step1 = {\"user_prompt\": \"Suggest a destination and a restaurant.\"}\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.0.0\",\n",
    "                           template=prompt_step1, \n",
    "                           variables=variables_step1\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step1 = query_openai(user_prompt=prompt_step1, variables=variables_step1)\n",
    "\n",
    "print(\"=== USER Prompt Template (Step 1) ===\") \n",
    "print(prompt_step1) \n",
    "print(\"\\n=== Variables (Step 1) ===\") \n",
    "print(variables_step1)\n",
    "print(\"\\n=== Response (Step 1) ===\") \n",
    "print(response_step1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 add examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:29.183116Z",
     "start_time": "2025-03-13T22:42:14.919743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USER Prompt template (Step 2) ===\n",
      "\n",
      "    {user_prompt}\n",
      "    Examples:\n",
      "        {examples}\n",
      "\n",
      "\n",
      "=== Variables (Step 2) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n'}\n",
      "\n",
      "=== Response (Step 2) ===\n",
      "Input: \"Charlie: I'm looking for a place to enjoy some live music and good food.\"\n",
      "Output: \"For a great live music experience and delicious Southern cuisine, I recommend checking out The Blue Note Bistro.\"\n"
     ]
    }
   ],
   "source": [
    "prompt_step2 = \"\"\"\n",
    "    {user_prompt}\n",
    "    Examples:\n",
    "        {examples}\n",
    "\"\"\"\n",
    "\n",
    "# Examples\n",
    "examples_step2 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step2 = {\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step2\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.1.0\",\n",
    "                           template=prompt_step2, \n",
    "                           variables=variables_step2\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step2 = query_openai(user_prompt=prompt_step2, variables=variables_step2)\n",
    "\n",
    "print(\"=== USER Prompt template (Step 2) ===\") \n",
    "print(prompt_step2) \n",
    "print(\"\\n=== Variables (Step 2) ===\")\n",
    "print(variables_step2)\n",
    "print(\"\\n=== Response (Step 2) ===\") \n",
    "print(response_step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 add system prompt with constraints and examples and lighten the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:37.706204Z",
     "start_time": "2025-03-13T22:42:29.198759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 3) ===\n",
      "\n",
      "            You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
      "            Here are some rules and examples:\n",
      "\n",
      "            - Always base your responses on user preferences.\n",
      "            - Do not provide answers beyond the scope of the context given.\n",
      "\n",
      "            Example exchanges:\n",
      "            {examples}\n",
      "\n",
      "            Use these examples to format your responses. \n",
      "\n",
      "            {constraints}\n",
      "\n",
      "            Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 3) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 3) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 3) ===\n",
      "I'm sorry, but I don't have enough information about your preferences to make a recommendation for a destination or a restaurant.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_step3 = \"\"\"\n",
    "            You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
    "            Here are some rules and examples:\n",
    "\n",
    "            - Always base your responses on user preferences.\n",
    "            - Do not provide answers beyond the scope of the context given.\n",
    "\n",
    "            Example exchanges:\n",
    "            {examples}\n",
    "\n",
    "            Use these examples to format your responses. \n",
    "\n",
    "            {constraints}\n",
    "\n",
    "            Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step3 = \"{user_prompt}\"\n",
    "\n",
    "examples_step3 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step3 = {\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step3, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"2.0.0\",\n",
    "                           chat_template=[(\"System\",system_prompt_step3),(\"User\", prompt_step3)], \n",
    "                           variables=variables_step3\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step3 = query_openai(system_prompt=system_prompt_step3, user_prompt=prompt_step3, variables=variables_step3)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 3) ===\")\n",
    "print(system_prompt_step3)\n",
    "print(\"=== USER Prompt template (Step 3) ===\") \n",
    "print(prompt_step3)\n",
    "print(\"\\n=== Variables (Step 3) ===\")\n",
    "print(variables_step3)\n",
    "print(\"\\n=== Response (Step 3) ===\") \n",
    "print(response_step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 add context to system prompt and name of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:53.447982Z",
     "start_time": "2025-03-13T22:42:37.726832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 4) ===\n",
      "\n",
      "        You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
      "        Here are some rules and examples:\n",
      "\n",
      "        - Always base your responses on user preferences.\n",
      "        - Do not provide answers beyond the scope of the context given.\n",
      "\n",
      "        Example exchanges:\n",
      "        {examples}\n",
      "\n",
      "        Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
      "        {name}'s Context: {context}\n",
      "\n",
      "        Use these examples to format your responses. \n",
      "\n",
      "        {constraints}\n",
      "\n",
      "        Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 4) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 4) ===\n",
      "{'name': 'Alice', 'context': 'Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.', 'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 4) ===\n",
      "Considering your love for beaches and mountains, I recommend visiting the Amalfi Coast in Italy for a stunning blend of both. For a vegetarian dining experience, you might enjoy La Caravella, known for its exquisite vegetarian options and beautiful coastal views.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_step4 = \"\"\"\n",
    "        You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
    "        Here are some rules and examples:\n",
    "\n",
    "        - Always base your responses on user preferences.\n",
    "        - Do not provide answers beyond the scope of the context given.\n",
    "\n",
    "        Example exchanges:\n",
    "        {examples}\n",
    "\n",
    "        Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
    "        {name}'s Context: {context}\n",
    "\n",
    "        Use these examples to format your responses. \n",
    "\n",
    "        {constraints}\n",
    "\n",
    "        Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step4 = \"{user_prompt}\"\n",
    "\n",
    "examples_step4 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step4 = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"context\": \"Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.\",\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step4, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"2.1.0\",\n",
    "                           chat_template=[(\"System\",system_prompt_step4),(\"User\", prompt_step4)], \n",
    "                           variables=variables_step4\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step4 = query_openai(system_prompt=system_prompt_step4, user_prompt=prompt_step4, variables=variables_step4)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 4) ===\")\n",
    "print(system_prompt_step4)\n",
    "print(\"=== USER Prompt template (Step 4) ===\") \n",
    "print(prompt_step4)\n",
    "print(\"\\n=== Variables (Step 4) ===\")\n",
    "print(variables_step4)\n",
    "print(\"\\n=== Response (Step 4) ===\") \n",
    "print(response_step4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:43:10.724377Z",
     "start_time": "2025-03-13T22:42:53.472982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 5) ===\n",
      "\n",
      "    You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences while maintaining the given context. Here are the rules:\n",
      "    - Base recommendations primarily on user preferences.\n",
      "    - Respect and adhere strictly to the context boundaries; do not go beyond the provided scope.\n",
      "    - Personalization is key to every response.\n",
      "\n",
      "    Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
      "        {name}'s Context: {context}\n",
      "\n",
      "    Example exchanges: {examples}\n",
      "\n",
      "    Apply these examples to guide your response format, ensuring a focus on personal preferences within context. {constraints} \n",
      "    Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 5) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 5) ===\n",
      "{'name': 'Alice', 'context': 'Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.', 'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 5) ===\n",
      "Considering your love for traveling to beaches and mountains, I recommend visiting the picturesque beaches of Maui, Hawaii. For a delightful vegetarian meal, you might enjoy the offerings at Choice Health Bar, which specializes in fresh, locally-sourced vegetarian dishes.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_step5 = \"\"\"\n",
    "    You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences while maintaining the given context. Here are the rules:\n",
    "    - Base recommendations primarily on user preferences.\n",
    "    - Respect and adhere strictly to the context boundaries; do not go beyond the provided scope.\n",
    "    - Personalization is key to every response.\n",
    "\n",
    "    Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
    "        {name}'s Context: {context}\n",
    "\n",
    "    Example exchanges: {examples}\n",
    "\n",
    "    Apply these examples to guide your response format, ensuring a focus on personal preferences within context. {constraints} \n",
    "    Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step5 = \"{user_prompt}\"\n",
    "\n",
    "examples_step5 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step5 = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"context\": \"Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.\",\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step5, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(\n",
    "                template=\"{var1} {var3}\",\n",
    "                variables={\"var1\": \"var1\", \"var2\": \"var3\"},\n",
    "                version=\"1.0.0\",\n",
    "                id=\"test_prompt\",\n",
    "                rag_context_variables=[\"var1\", \"var2\"],\n",
    "                rag_query_variables=[\"user_input\"],\n",
    "            )):\n",
    "    for i in range(10):\n",
    "        response_step5 = query_openai(system_prompt=system_prompt_step5, user_prompt=prompt_step5, variables=variables_step5)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 5) ===\")\n",
    "print(system_prompt_step5)\n",
    "print(\"=== USER Prompt template (Step 5) ===\") \n",
    "print(prompt_step5)\n",
    "print(\"\\n=== Variables (Step 5) ===\")\n",
    "print(variables_step5)\n",
    "print(\"\\n=== Response (Step 5) ===\") \n",
    "print(response_step5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:43:10.757207Z",
     "start_time": "2025-03-13T22:43:10.755238Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
