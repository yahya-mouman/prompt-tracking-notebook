{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:30.592891Z",
     "start_time": "2025-03-13T22:41:30.066339Z"
    }
   },
   "source": [
    "pip install openai"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (1.65.4)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai) (4.8.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.28.1)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.8.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai) (2.10.6)\r\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai) (4.12.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\r\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:31.105282Z",
     "start_time": "2025-03-13T22:41:30.597560Z"
    }
   },
   "source": [
    "pip install dotenv"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in ./.venv/lib/python3.12/site-packages (0.9.9)\r\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from dotenv) (1.0.1)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:47.658114Z",
     "start_time": "2025-03-13T22:41:31.174098Z"
    }
   },
   "source": [
    "pip install git+https://github.com/DataDog/dd-trace-py.git@yahya/update-prompt-annotation"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/DataDog/dd-trace-py.git@yahya/update-prompt-annotation\r\n",
      "  Cloning https://github.com/DataDog/dd-trace-py.git (to revision yahya/update-prompt-annotation) to /private/var/folders/fw/0099t8y529b2x_y472l_vnr40000gp/T/pip-req-build-0x5izpz9\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/DataDog/dd-trace-py.git /private/var/folders/fw/0099t8y529b2x_y472l_vnr40000gp/T/pip-req-build-0x5izpz9\r\n",
      "  Running command git checkout -b yahya/update-prompt-annotation --track origin/yahya/update-prompt-annotation\r\n",
      "  Switched to a new branch 'yahya/update-prompt-annotation'\r\n",
      "  branch 'yahya/update-prompt-annotation' set up to track 'origin/yahya/update-prompt-annotation'.\r\n",
      "  Resolved https://github.com/DataDog/dd-trace-py.git to commit 6019952bd630716080fd2562ab86304287955dfd\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: bytecode>=0.15.1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.2.0.dev29+g6019952bd) (0.16.1)\r\n",
      "Requirement already satisfied: envier~=0.6.1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.2.0.dev29+g6019952bd) (0.6.1)\r\n",
      "Requirement already satisfied: opentelemetry-api>=1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.2.0.dev29+g6019952bd) (1.30.0)\r\n",
      "Requirement already satisfied: protobuf>=3 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.2.0.dev29+g6019952bd) (6.30.0)\r\n",
      "Requirement already satisfied: typing_extensions in ./.venv/lib/python3.12/site-packages (from ddtrace==3.2.0.dev29+g6019952bd) (4.12.2)\r\n",
      "Requirement already satisfied: xmltodict>=0.12 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.2.0.dev29+g6019952bd) (0.14.2)\r\n",
      "Requirement already satisfied: wrapt>=1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.2.0.dev29+g6019952bd) (1.17.2)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1->ddtrace==3.2.0.dev29+g6019952bd) (1.2.18)\r\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1->ddtrace==3.2.0.dev29+g6019952bd) (8.5.0)\r\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1->ddtrace==3.2.0.dev29+g6019952bd) (3.21.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:48.038521Z",
     "start_time": "2025-03-13T22:41:47.663029Z"
    }
   },
   "source": [
    "from ddtrace.llmobs import LLMObs\n",
    "from ddtrace.llmobs.utils import Prompt\n",
    "from ddtrace import patch\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "patch(openai=True)\n",
    "\n",
    "openai.api_key = os.environ.get(\"openai_api_key\")\n",
    "client = OpenAI()\n",
    "load_dotenv()\n",
    "\n",
    "LLMObs.enable(\n",
    "    api_key=os.environ.get(\"dd_api_key\"),\n",
    "    site=os.environ.get(\"dd_site\"),\n",
    "    ml_app=\"prompt-tracking-sandbox-v2\",\n",
    "    service=\"prompt-tracking-sandbox-v2\",\n",
    "    agentless_enabled=True,\n",
    "    env=\"testing\",\n",
    "    integrations_enabled=False,\n",
    ")\n",
    "\n",
    "prompt_id = \"prompt-tracking-sandbox-prompt\"\n",
    "prompt_name = \"recommendation_engine\"\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:48.045934Z",
     "start_time": "2025-03-13T22:41:48.043179Z"
    }
   },
   "source": [
    "def query_openai(system_prompt=None, user_prompt=None, variables=[], model=\"gpt-4o\", max_tokens=100, temperature=0.7):\n",
    "     \"\"\" Sends a prompt to the OpenAI API and returns the text response. \"\"\" \n",
    "     if system_prompt and user_prompt:\n",
    "          system_prompt_rendered = system_prompt.format(**variables)\n",
    "          user_prompt_rendered = user_prompt.format(**variables)\n",
    "          completion = client.chat.completions.create(\n",
    "               model=model,\n",
    "               max_tokens=max_tokens,\n",
    "               temperature=temperature,\n",
    "               messages=[{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_rendered\n",
    "                    },\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt_rendered\n",
    "                    }]\n",
    "          )\n",
    "     elif user_prompt:\n",
    "          user_prompt_rendered = user_prompt.format(**variables)\n",
    "          completion = client.chat.completions.create(\n",
    "               model=model,\n",
    "               max_tokens=max_tokens,\n",
    "               temperature=temperature,\n",
    "               messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt_rendered\n",
    "                    }]\n",
    "          )\n",
    "     elif system_prompt:\n",
    "            system_prompt_rendered = system_prompt.format(**variables)\n",
    "            completion = client.chat.completions.create(\n",
    "                 model=model,\n",
    "                 max_tokens=max_tokens,\n",
    "                 temperature=temperature,\n",
    "                 messages=[{\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt_rendered\n",
    "                        }]\n",
    "            )\n",
    "     return completion.choices[0].message.content"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to annotate a prompt to a span \n",
    "In order to trace the prompt we wrap the openai call with LLMObs.prompt_context or LLMObs.annotation_context.\n",
    "Example :\n",
    "\n",
    "\"\"\"\n",
    "with LLMObs.prompt_context(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.0.0\",\n",
    "                           template=[(\"User\", prompt_step1)], \n",
    "                           variables=variables_step1\n",
    "                           ):\n",
    "\"\"\"\n",
    "Or\n",
    "\"\"\"\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id,\n",
    "                           name=prompt_name,\n",
    "                           version=\"1.0.0\",\n",
    "                           template=[(\"User\", prompt_step1)],\n",
    "                           variables=variables_step1\n",
    "                           )):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 simple user prompt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:14.910406Z",
     "start_time": "2025-03-13T22:41:48.050376Z"
    }
   },
   "source": [
    "prompt_step1 = \"{user_prompt}\"\n",
    "variables_step1 = {\"user_prompt\": \"Suggest a destination and a restaurant.\"}\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.0.0\",\n",
    "                           template=[(\"User\", prompt_step1)], \n",
    "                           variables=variables_step1\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step1 = query_openai(user_prompt=prompt_step1, variables=variables_step1)\n",
    "\n",
    "print(\"=== USER Prompt Template (Step 1) ===\") \n",
    "print(prompt_step1) \n",
    "print(\"\\n=== Variables (Step 1) ===\") \n",
    "print(variables_step1)\n",
    "print(\"\\n=== Response (Step 1) ===\") \n",
    "print(response_step1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USER Prompt Template (Step 1) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 1) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.'}\n",
      "\n",
      "=== Response (Step 1) ===\n",
      "For a delightful travel experience, consider visiting Kyoto, Japan. Known for its stunning temples, traditional wooden houses, and beautiful gardens, Kyoto offers a serene and culturally rich atmosphere.\n",
      "\n",
      "While in Kyoto, dine at \"Kikunoi,\" a renowned kaiseki restaurant. Kikunoi offers an exquisite multi-course dining experience that showcases the best of Japanese seasonal ingredients and culinary artistry. The elegant presentation and harmonious flavors provide an unforgettable taste of Kyoto's culinary heritage. Be sure to make a reservation in advance to\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 add examples"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:29.183116Z",
     "start_time": "2025-03-13T22:42:14.919743Z"
    }
   },
   "source": [
    "prompt_step2 = \"\"\"\n",
    "    {user_prompt}\n",
    "    Examples:\n",
    "        {examples}\n",
    "\"\"\"\n",
    "\n",
    "# Examples\n",
    "examples_step2 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step2 = {\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step2\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.1.0\",\n",
    "                           template=[(\"User\", prompt_step2)], \n",
    "                           variables=variables_step2\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step2 = query_openai(user_prompt=prompt_step2, variables=variables_step2)\n",
    "\n",
    "print(\"=== USER Prompt template (Step 2) ===\") \n",
    "print(prompt_step2) \n",
    "print(\"\\n=== Variables (Step 2) ===\")\n",
    "print(variables_step2)\n",
    "print(\"\\n=== Response (Step 2) ===\") \n",
    "print(response_step2)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USER Prompt template (Step 2) ===\n",
      "\n",
      "    {user_prompt}\n",
      "    Examples:\n",
      "        {examples}\n",
      "\n",
      "\n",
      "=== Variables (Step 2) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n'}\n",
      "\n",
      "=== Response (Step 2) ===\n",
      "Input: \"Sam: Where should I go for a cultural experience?\"  \n",
      "Output: \"Given your interest in cultural experiences, I recommend visiting Kyoto, Japan, and dining at Kikunoi for an authentic kaiseki meal.\"\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 add system prompt with constraints and examples and lighten the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:37.706204Z",
     "start_time": "2025-03-13T22:42:29.198759Z"
    }
   },
   "source": [
    "system_prompt_step3 = \"\"\"\n",
    "            You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
    "            Here are some rules and examples:\n",
    "\n",
    "            - Always base your responses on user preferences.\n",
    "            - Do not provide answers beyond the scope of the context given.\n",
    "\n",
    "            Example exchanges:\n",
    "            {examples}\n",
    "\n",
    "            Use these examples to format your responses. \n",
    "\n",
    "            {constraints}\n",
    "\n",
    "            Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step3 = \"{user_prompt}\"\n",
    "\n",
    "examples_step3 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step3 = {\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step3, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"2.0.0\",\n",
    "                           template=[(\"System\",system_prompt_step3),(\"User\", prompt_step3)], \n",
    "                           variables=variables_step3\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step3 = query_openai(system_prompt=system_prompt_step3, user_prompt=prompt_step3, variables=variables_step3)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 3) ===\")\n",
    "print(system_prompt_step3)\n",
    "print(\"=== USER Prompt template (Step 3) ===\") \n",
    "print(prompt_step3)\n",
    "print(\"\\n=== Variables (Step 3) ===\")\n",
    "print(variables_step3)\n",
    "print(\"\\n=== Response (Step 3) ===\") \n",
    "print(response_step3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 3) ===\n",
      "\n",
      "            You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
      "            Here are some rules and examples:\n",
      "\n",
      "            - Always base your responses on user preferences.\n",
      "            - Do not provide answers beyond the scope of the context given.\n",
      "\n",
      "            Example exchanges:\n",
      "            {examples}\n",
      "\n",
      "            Use these examples to format your responses. \n",
      "\n",
      "            {constraints}\n",
      "\n",
      "            Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 3) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 3) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 3) ===\n",
      "I'm unable to provide a recommendation as there isn't specific information about your preferences for destinations or restaurants.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 add context to system prompt and name of the user"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:53.447982Z",
     "start_time": "2025-03-13T22:42:37.726832Z"
    }
   },
   "source": [
    "system_prompt_step4 = \"\"\"\n",
    "        You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
    "        Here are some rules and examples:\n",
    "\n",
    "        - Always base your responses on user preferences.\n",
    "        - Do not provide answers beyond the scope of the context given.\n",
    "\n",
    "        Example exchanges:\n",
    "        {examples}\n",
    "\n",
    "        Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
    "        {name}'s Context: {context}\n",
    "\n",
    "        Use these examples to format your responses. \n",
    "\n",
    "        {constraints}\n",
    "\n",
    "        Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step4 = \"{user_prompt}\"\n",
    "\n",
    "examples_step4 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step4 = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"context\": \"Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.\",\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step4, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"2.1.0\",\n",
    "                           template=[(\"System\",system_prompt_step4),(\"User\", prompt_step4)], \n",
    "                           variables=variables_step4\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step4 = query_openai(system_prompt=system_prompt_step4, user_prompt=prompt_step4, variables=variables_step4)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 4) ===\")\n",
    "print(system_prompt_step4)\n",
    "print(\"=== USER Prompt template (Step 4) ===\") \n",
    "print(prompt_step4)\n",
    "print(\"\\n=== Variables (Step 4) ===\")\n",
    "print(variables_step4)\n",
    "print(\"\\n=== Response (Step 4) ===\") \n",
    "print(response_step4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 4) ===\n",
      "\n",
      "        You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
      "        Here are some rules and examples:\n",
      "\n",
      "        - Always base your responses on user preferences.\n",
      "        - Do not provide answers beyond the scope of the context given.\n",
      "\n",
      "        Example exchanges:\n",
      "        {examples}\n",
      "\n",
      "        Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
      "        {name}'s Context: {context}\n",
      "\n",
      "        Use these examples to format your responses. \n",
      "\n",
      "        {constraints}\n",
      "\n",
      "        Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 4) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 4) ===\n",
      "{'name': 'Alice', 'context': 'Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.', 'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 4) ===\n",
      "Considering your love for beaches and mountains, I recommend visiting Maui, Hawaii, for its beautiful beaches and scenic mountain views. For a dining option, you might enjoy the vegetarian offerings at Choice Health Bar, which aligns with your preference for vegetarian food.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:43:10.724377Z",
     "start_time": "2025-03-13T22:42:53.472982Z"
    }
   },
   "source": [
    "system_prompt_step5 = \"\"\"\n",
    "    You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences while maintaining the given context. Here are the rules:\n",
    "    - Base recommendations primarily on user preferences.\n",
    "    - Respect and adhere strictly to the context boundaries; do not go beyond the provided scope.\n",
    "    - Personalization is key to every response.\n",
    "\n",
    "    Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
    "        {name}'s Context: {context}\n",
    "\n",
    "    Example exchanges: {examples}\n",
    "\n",
    "    Apply these examples to guide your response format, ensuring a focus on personal preferences within context. {constraints} \n",
    "    Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step5 = \"{user_prompt}\"\n",
    "\n",
    "examples_step5 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step5 = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"context\": \"Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.\",\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step5, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"2.2.0\",\n",
    "                           template=[(\"System\",system_prompt_step5),(\"User\", prompt_step5)], \n",
    "                           variables=variables_step5\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step5 = query_openai(system_prompt=system_prompt_step5, user_prompt=prompt_step5, variables=variables_step5)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 5) ===\")\n",
    "print(system_prompt_step5)\n",
    "print(\"=== USER Prompt template (Step 5) ===\") \n",
    "print(prompt_step5)\n",
    "print(\"\\n=== Variables (Step 5) ===\")\n",
    "print(variables_step5)\n",
    "print(\"\\n=== Response (Step 5) ===\") \n",
    "print(response_step5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 5) ===\n",
      "\n",
      "    You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences while maintaining the given context. Here are the rules:\n",
      "    - Base recommendations primarily on user preferences.\n",
      "    - Respect and adhere strictly to the context boundaries; do not go beyond the provided scope.\n",
      "    - Personalization is key to every response.\n",
      "\n",
      "    Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
      "        {name}'s Context: {context}\n",
      "\n",
      "    Example exchanges: {examples}\n",
      "\n",
      "    Apply these examples to guide your response format, ensuring a focus on personal preferences within context. {constraints} \n",
      "    Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 5) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 5) ===\n",
      "{'name': 'Alice', 'context': 'Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.', 'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 5) ===\n",
      "Considering your love for beaches and mountains, I recommend visiting Kauai, Hawaii. It's known for its stunning landscapes, with both beautiful beaches and majestic mountains. For a dining experience that aligns with your vegetarian preference, try The Greenery Café, which offers a variety of delicious vegetarian dishes in a serene setting.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:43:10.757207Z",
     "start_time": "2025-03-13T22:43:10.755238Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
