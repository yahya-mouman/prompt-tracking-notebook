{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:30.592891Z",
     "start_time": "2025-03-13T22:41:30.066339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.12/site-packages (1.65.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.12/site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.venv/lib/python3.12/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:31.105282Z",
     "start_time": "2025-03-13T22:41:30.597560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in ./.venv/lib/python3.12/site-packages (0.9.9)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (from dotenv) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:47.658114Z",
     "start_time": "2025-03-13T22:41:31.174098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/DataDog/dd-trace-py.git@yahya/update-prompt-annotation\n",
      "  Cloning https://github.com/DataDog/dd-trace-py.git (to revision yahya/update-prompt-annotation) to /private/var/folders/fw/0099t8y529b2x_y472l_vnr40000gp/T/pip-req-build-zcaaz_1v\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/DataDog/dd-trace-py.git /private/var/folders/fw/0099t8y529b2x_y472l_vnr40000gp/T/pip-req-build-zcaaz_1v\n",
      "  Running command git checkout -b yahya/update-prompt-annotation --track origin/yahya/update-prompt-annotation\n",
      "  Switched to a new branch 'yahya/update-prompt-annotation'\n",
      "  branch 'yahya/update-prompt-annotation' set up to track 'origin/yahya/update-prompt-annotation'.\n",
      "  Resolved https://github.com/DataDog/dd-trace-py.git to commit 9be91f98182cde99fbded50f6fb6d0852557aee1\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bytecode>=0.15.1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev120+g9be91f981) (0.16.1)\n",
      "Requirement already satisfied: envier~=0.6.1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev120+g9be91f981) (0.6.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev120+g9be91f981) (1.30.0)\n",
      "Requirement already satisfied: protobuf>=3 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev120+g9be91f981) (6.30.0)\n",
      "Requirement already satisfied: typing_extensions in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev120+g9be91f981) (4.12.2)\n",
      "Requirement already satisfied: xmltodict>=0.12 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev120+g9be91f981) (0.14.2)\n",
      "Requirement already satisfied: wrapt>=1 in ./.venv/lib/python3.12/site-packages (from ddtrace==3.4.0.dev120+g9be91f981) (1.17.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1->ddtrace==3.4.0.dev120+g9be91f981) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in ./.venv/lib/python3.12/site-packages (from opentelemetry-api>=1->ddtrace==3.4.0.dev120+g9be91f981) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1->ddtrace==3.4.0.dev120+g9be91f981) (3.21.0)\n",
      "Building wheels for collected packages: ddtrace\n",
      "  Building wheel for ddtrace (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ddtrace: filename=ddtrace-3.4.0.dev120+g9be91f981-cp312-cp312-macosx_14_0_arm64.whl size=8541283 sha256=f36633be37b477db4e07927a20fe850c27752e8b29dd23a2f48c3308f53812d6\n",
      "  Stored in directory: /private/var/folders/fw/0099t8y529b2x_y472l_vnr40000gp/T/pip-ephem-wheel-cache-sop8h9zh/wheels/55/fd/11/cfb8de599ac245987c278626cc41e148a13c4b25bb618ed43b\n",
      "Successfully built ddtrace\n",
      "Installing collected packages: ddtrace\n",
      "  Attempting uninstall: ddtrace\n",
      "    Found existing installation: ddtrace 3.4.0.dev109+gd145d6828\n",
      "    Uninstalling ddtrace-3.4.0.dev109+gd145d6828:\n",
      "      Successfully uninstalled ddtrace-3.4.0.dev109+gd145d6828\n",
      "Successfully installed ddtrace-3.4.0.dev120+g9be91f981\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/DataDog/dd-trace-py.git@yahya/update-prompt-annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:48.038521Z",
     "start_time": "2025-03-13T22:41:47.663029Z"
    }
   },
   "outputs": [],
   "source": [
    "from ddtrace.llmobs import LLMObs\n",
    "from ddtrace.llmobs.utils import Prompt\n",
    "from ddtrace import patch\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "patch(openai=True)\n",
    "\n",
    "openai.api_key = os.environ.get(\"openai_api_key\")\n",
    "client = OpenAI()\n",
    "load_dotenv()\n",
    "\n",
    "LLMObs.enable(\n",
    "    api_key=os.environ.get(\"dd_api_key\"),\n",
    "    site=os.environ.get(\"dd_site\"),\n",
    "    ml_app=\"prompt-tracking-sandbox-v3\",\n",
    "    service=\"prompt-tracking-sandbox-v3\",\n",
    "    agentless_enabled=True,\n",
    "    env=\"testing\",\n",
    "    integrations_enabled=False,\n",
    ")\n",
    "\n",
    "prompt_id = \"prompt-tracking-sandbox-prompt\"\n",
    "prompt_name = \"recommendation_engine\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:41:48.045934Z",
     "start_time": "2025-03-13T22:41:48.043179Z"
    }
   },
   "outputs": [],
   "source": [
    "def query_openai(system_prompt=None, user_prompt=None, variables=[], model=\"gpt-4o\", max_tokens=100, temperature=0.7):\n",
    "     \"\"\" Sends a prompt to the OpenAI API and returns the text response. \"\"\" \n",
    "     if system_prompt and user_prompt:\n",
    "          system_prompt_rendered = system_prompt.format(**variables)\n",
    "          user_prompt_rendered = user_prompt.format(**variables)\n",
    "          completion = client.chat.completions.create(\n",
    "               model=model,\n",
    "               max_tokens=max_tokens,\n",
    "               temperature=temperature,\n",
    "               messages=[{\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt_rendered\n",
    "                    },\n",
    "                    {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt_rendered\n",
    "                    }]\n",
    "          )\n",
    "     elif user_prompt:\n",
    "          user_prompt_rendered = user_prompt.format(**variables)\n",
    "          completion = client.chat.completions.create(\n",
    "               model=model,\n",
    "               max_tokens=max_tokens,\n",
    "               temperature=temperature,\n",
    "               messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_prompt_rendered\n",
    "                    }]\n",
    "          )\n",
    "     elif system_prompt:\n",
    "            system_prompt_rendered = system_prompt.format(**variables)\n",
    "            completion = client.chat.completions.create(\n",
    "                 model=model,\n",
    "                 max_tokens=max_tokens,\n",
    "                 temperature=temperature,\n",
    "                 messages=[{\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt_rendered\n",
    "                        }]\n",
    "            )\n",
    "     return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to annotate a prompt to a span \n",
    "In order to trace the prompt we wrap the openai call with LLMObs.prompt_context or LLMObs.annotation_context.\n",
    "Example :\n",
    "\n",
    "\"\"\"\n",
    "with LLMObs.prompt_context(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.0.0\",\n",
    "                           template=[(\"User\", prompt_step1)], \n",
    "                           variables=variables_step1\n",
    "                           ):\n",
    "\"\"\"\n",
    "Or\n",
    "\"\"\"\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id,\n",
    "                           name=prompt_name,\n",
    "                           version=\"1.0.0\",\n",
    "                           template=[(\"User\", prompt_step1)],\n",
    "                           variables=variables_step1\n",
    "                           )):\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 simple user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:14.910406Z",
     "start_time": "2025-03-13T22:41:48.050376Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USER Prompt Template (Step 1) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 1) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.'}\n",
      "\n",
      "=== Response (Step 1) ===\n",
      "**Destination: Kyoto, Japan**\n",
      "\n",
      "Kyoto is a city that beautifully blends the ancient with the modern. With its stunning temples, traditional tea houses, and picturesque gardens, it offers a glimpse into Japan's rich cultural heritage. The city's charming streets and seasonal beauty, especially during cherry blossom and autumn foliage, make it a captivating destination.\n",
      "\n",
      "**Restaurant: Kikunoi Honten**\n",
      "\n",
      "While in Kyoto, indulge in a dining experience at Kikunoi Honten, a renowned kaiseki restaurant. Kais\n"
     ]
    }
   ],
   "source": [
    "prompt_step1 = \"{user_prompt}\"\n",
    "variables_step1 = {\"user_prompt\": \"Suggest a destination and a restaurant.\"}\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.0.0\",\n",
    "                           template=prompt_step1, \n",
    "                           variables=variables_step1\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step1 = query_openai(user_prompt=prompt_step1, variables=variables_step1)\n",
    "\n",
    "print(\"=== USER Prompt Template (Step 1) ===\") \n",
    "print(prompt_step1) \n",
    "print(\"\\n=== Variables (Step 1) ===\") \n",
    "print(variables_step1)\n",
    "print(\"\\n=== Response (Step 1) ===\") \n",
    "print(response_step1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 add examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:29.183116Z",
     "start_time": "2025-03-13T22:42:14.919743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== USER Prompt template (Step 2) ===\n",
      "\n",
      "    {user_prompt}\n",
      "    Examples:\n",
      "        {examples}\n",
      "\n",
      "\n",
      "=== Variables (Step 2) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n'}\n",
      "\n",
      "=== Response (Step 2) ===\n",
      "Input: \"Charlie: I'm looking for a great city to explore and a nice place to eat.\"\n",
      "Output: \"Given your interest in vibrant city life, I suggest exploring Barcelona, and for a delightful meal, try out Tickets Bar for a unique tapas experience.\"\n"
     ]
    }
   ],
   "source": [
    "prompt_step2 = \"\"\"\n",
    "    {user_prompt}\n",
    "    Examples:\n",
    "        {examples}\n",
    "\"\"\"\n",
    "\n",
    "# Examples\n",
    "examples_step2 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step2 = {\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step2\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"1.1.0\",\n",
    "                           template=prompt_step2, \n",
    "                           variables=variables_step2\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step2 = query_openai(user_prompt=prompt_step2, variables=variables_step2)\n",
    "\n",
    "print(\"=== USER Prompt template (Step 2) ===\") \n",
    "print(prompt_step2) \n",
    "print(\"\\n=== Variables (Step 2) ===\")\n",
    "print(variables_step2)\n",
    "print(\"\\n=== Response (Step 2) ===\") \n",
    "print(response_step2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 add system prompt with constraints and examples and lighten the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:37.706204Z",
     "start_time": "2025-03-13T22:42:29.198759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 3) ===\n",
      "\n",
      "            You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
      "            Here are some rules and examples:\n",
      "\n",
      "            - Always base your responses on user preferences.\n",
      "            - Do not provide answers beyond the scope of the context given.\n",
      "\n",
      "            Example exchanges:\n",
      "            {examples}\n",
      "\n",
      "            Use these examples to format your responses. \n",
      "\n",
      "            {constraints}\n",
      "\n",
      "            Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 3) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 3) ===\n",
      "{'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 3) ===\n",
      "Based on your preferences for beach destinations and seafood cuisine, I recommend visiting Maui, Hawaii, and dining at Mama's Fish House.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_step3 = \"\"\"\n",
    "            You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
    "            Here are some rules and examples:\n",
    "\n",
    "            - Always base your responses on user preferences.\n",
    "            - Do not provide answers beyond the scope of the context given.\n",
    "\n",
    "            Example exchanges:\n",
    "            {examples}\n",
    "\n",
    "            Use these examples to format your responses. \n",
    "\n",
    "            {constraints}\n",
    "\n",
    "            Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step3 = \"{user_prompt}\"\n",
    "\n",
    "examples_step3 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step3 = {\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step3, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"2.0.0\",\n",
    "                           chat_template=[{\"role\":\"System\",\"content\":system_prompt_step3},{\"role\":\"User\", \"content\":prompt_step3}], \n",
    "                           variables=variables_step3\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step3 = query_openai(system_prompt=system_prompt_step3, user_prompt=prompt_step3, variables=variables_step3)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 3) ===\")\n",
    "print(system_prompt_step3)\n",
    "print(\"=== USER Prompt template (Step 3) ===\") \n",
    "print(prompt_step3)\n",
    "print(\"\\n=== Variables (Step 3) ===\")\n",
    "print(variables_step3)\n",
    "print(\"\\n=== Response (Step 3) ===\") \n",
    "print(response_step3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 add context to system prompt and name of the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:42:53.447982Z",
     "start_time": "2025-03-13T22:42:37.726832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 4) ===\n",
      "\n",
      "        You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
      "        Here are some rules and examples:\n",
      "\n",
      "        - Always base your responses on user preferences.\n",
      "        - Do not provide answers beyond the scope of the context given.\n",
      "\n",
      "        Example exchanges:\n",
      "        {examples}\n",
      "\n",
      "        Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
      "        {name}'s Context: {context}\n",
      "\n",
      "        Use these examples to format your responses. \n",
      "\n",
      "        {constraints}\n",
      "\n",
      "        Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 4) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 4) ===\n",
      "{'name': 'Alice', 'context': 'Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.', 'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 4) ===\n",
      "Considering your love for beaches and mountains, I recommend visiting the beautiful beaches of Maui, Hawaii. For a delightful vegetarian meal, you might enjoy dining at Choice Health Bar, which offers a variety of vegetarian options.\n"
     ]
    }
   ],
   "source": [
    "system_prompt_step4 = \"\"\"\n",
    "        You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences.\n",
    "        Here are some rules and examples:\n",
    "\n",
    "        - Always base your responses on user preferences.\n",
    "        - Do not provide answers beyond the scope of the context given.\n",
    "\n",
    "        Example exchanges:\n",
    "        {examples}\n",
    "\n",
    "        Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
    "        {name}'s Context: {context}\n",
    "\n",
    "        Use these examples to format your responses. \n",
    "\n",
    "        {constraints}\n",
    "\n",
    "        Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step4 = \"{user_prompt}\"\n",
    "\n",
    "examples_step4 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step4 = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"context\": \"Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.\",\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step4, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"2.1.0\",\n",
    "                           chat_template=[{\"role\":\"System\",\"content\":system_prompt_step4},{\"role\":\"User\", \"content\":prompt_step4}], \n",
    "                           variables=variables_step4\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step4 = query_openai(system_prompt=system_prompt_step4, user_prompt=prompt_step4, variables=variables_step4)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 4) ===\")\n",
    "print(system_prompt_step4)\n",
    "print(\"=== USER Prompt template (Step 4) ===\") \n",
    "print(prompt_step4)\n",
    "print(\"\\n=== Variables (Step 4) ===\")\n",
    "print(variables_step4)\n",
    "print(\"\\n=== Response (Step 4) ===\") \n",
    "print(response_step4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:43:10.724377Z",
     "start_time": "2025-03-13T22:42:53.472982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM Prompt template (Step 5) ===\n",
      "\n",
      "    You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences while maintaining the given context. Here are the rules:\n",
      "    - Base recommendations primarily on user preferences.\n",
      "    - Respect and adhere strictly to the context boundaries; do not go beyond the provided scope.\n",
      "    - Personalization is key to every response.\n",
      "\n",
      "    Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
      "        {name}'s Context: {context}\n",
      "\n",
      "    Example exchanges: {examples}\n",
      "\n",
      "    Apply these examples to guide your response format, ensuring a focus on personal preferences within context. {constraints} \n",
      "    Now, here's your specific task:\n",
      "    \n",
      "=== USER Prompt template (Step 5) ===\n",
      "{user_prompt}\n",
      "\n",
      "=== Variables (Step 5) ===\n",
      "{'name': 'Alice', 'context': 'Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.', 'user_prompt': 'Suggest a destination and a restaurant.', 'examples': '\\nExample 1:\\nInput: \"Bob: Can you suggest a restaurant?\"\\nOutput: \"Based on your preferences for Italian cuisine, I recommend trying Luigi\\'s Trattoria.\"\\n\\nExample 2:\\nInput: \"Alice: What\\'s a good place for nature walks?\"\\nOutput: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\\n', 'constraints': 'Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.'}\n",
      "\n",
      "=== Response (Step 5) ===\n",
      "Based on your love for beaches and mountains, I recommend visiting the beautiful island of Maui, Hawaii. It offers stunning beaches and lush mountainous landscapes perfect for exploration. For a vegetarian dining experience, you might enjoy a meal at Moku Roots, a popular vegetarian restaurant in Lahaina, known for its delicious and creative plant-based dishes. Enjoy your trip!\n"
     ]
    }
   ],
   "source": [
    "system_prompt_step5 = \"\"\"\n",
    "    You are a recommendation bot with a focus on user preferences. Your main task is to provide the best recommendations based on provided user preferences while maintaining the given context. Here are the rules:\n",
    "    - Base recommendations primarily on user preferences.\n",
    "    - Respect and adhere strictly to the context boundaries; do not go beyond the provided scope.\n",
    "    - Personalization is key to every response.\n",
    "\n",
    "    Knowing the users' preferences is key to providing accurate recommendations. Here is the context for the current user:\n",
    "        {name}'s Context: {context}\n",
    "\n",
    "    Example exchanges: {examples}\n",
    "\n",
    "    Apply these examples to guide your response format, ensuring a focus on personal preferences within context. {constraints} \n",
    "    Now, here's your specific task:\n",
    "    \"\"\"\n",
    "\n",
    "prompt_step5 = \"{user_prompt}\"\n",
    "\n",
    "examples_step5 = \"\"\"\n",
    "Example 1:\n",
    "Input: \"Bob: Can you suggest a restaurant?\"\n",
    "Output: \"Based on your preferences for Italian cuisine, I recommend trying Luigi's Trattoria.\"\n",
    "\n",
    "Example 2:\n",
    "Input: \"Alice: What's a good place for nature walks?\"\n",
    "Output: \"Considering your love for scenic views, I suggest visiting Green Park Gardens.\"\n",
    "\"\"\"\n",
    "\n",
    "variables_step5 = {\n",
    "    \"name\": \"Alice\",\n",
    "    \"context\": \"Alice is allergic to peanuts and enjoys traveling to beaches and mountains. She prefers vegetarian food.\",\n",
    "    \"user_prompt\": \"Suggest a destination and a restaurant.\", \n",
    "    \"examples\": examples_step5, \n",
    "    \"constraints\": \"Do not ask the user for more information. Either provide a recommendation or state that data is insufficient for a recommendation.\"\n",
    "    }\n",
    "\n",
    "with LLMObs.annotation_context(prompt=Prompt(id=prompt_id, \n",
    "                           name=prompt_name, \n",
    "                           version=\"2.2.0\",\n",
    "                           chat_template=[{\"role\":\"System\",\"content\":system_prompt_step5},{\"role\":\"User\", \"content\":prompt_step5}], \n",
    "                           variables=variables_step5\n",
    "                           )):\n",
    "    for i in range(10):\n",
    "        response_step5 = query_openai(system_prompt=system_prompt_step5, user_prompt=prompt_step5, variables=variables_step5)\n",
    "\n",
    "print(\"=== SYSTEM Prompt template (Step 5) ===\")\n",
    "print(system_prompt_step5)\n",
    "print(\"=== USER Prompt template (Step 5) ===\") \n",
    "print(prompt_step5)\n",
    "print(\"\\n=== Variables (Step 5) ===\")\n",
    "print(variables_step5)\n",
    "print(\"\\n=== Response (Step 5) ===\") \n",
    "print(response_step5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T22:43:10.757207Z",
     "start_time": "2025-03-13T22:43:10.755238Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
